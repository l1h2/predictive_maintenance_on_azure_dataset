{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance with Azure Dataset\n",
    "\n",
    "## Project imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports\n",
    "\n",
    "Needs pre-processing.ipynb to be run first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(name: str, parse_dates: list[str] | None = [\"datetime\"]) -> pd.DataFrame:\n",
    "    path = \"data/\"\n",
    "    ext = \".csv\"\n",
    "    file = path + name + ext\n",
    "    return pd.read_csv(file, parse_dates=parse_dates, na_values=\"NaN\")\n",
    "\n",
    "VARIABLES = [\"volt\", \"rotate\", \"pressure\", \"vibration\"]\n",
    "DATA = read(\"raw_data\").dropna(subset=VARIABLES)\n",
    "normal_behavior_data = read(\"preprocessing/expected_behavior\")\n",
    "abnormal_data = read(\"preprocessing/failures_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset regression\n",
    "\n",
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877209, 8)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "independent_data = DATA.drop(columns=[\"datetime\", \"machineID\", \"failure\"])\n",
    "\n",
    "numeric_features = [\"volt\", \"rotate\", \"pressure\", \"vibration\", \"age\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"model\", \"errorID\", \"comp\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"None\")),\n",
    "        (\"ordinal\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessed_data = preprocessor.fit_transform(independent_data)\n",
    "print(preprocessed_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(876657, 1)\n",
      "(35585, 1)\n"
     ]
    }
   ],
   "source": [
    "time_before_failure = 0\n",
    "max_interval = 48\n",
    "last_failure = None\n",
    "\n",
    "labels = DATA[[\"failure\"]].fillna(\"normal\").reset_index(drop=True)\n",
    "\n",
    "for i in range(len(labels) - 1, -1, -1):\n",
    "    if labels.at[i, \"failure\"] != \"normal\":\n",
    "        time_before_failure = 0\n",
    "        labels.at[i, \"failure\"] = 0\n",
    "        if not last_failure:\n",
    "            last_failure = i + 1\n",
    "\n",
    "    else:\n",
    "        time_before_failure += 1\n",
    "        labels.at[i, \"failure\"] = time_before_failure\n",
    "\n",
    "\n",
    "labels = labels.iloc[:last_failure]\n",
    "short_labels = labels[labels[\"failure\"] <= max_interval]\n",
    "preprocessed_data = preprocessed_data[:last_failure]\n",
    "short_data = preprocessed_data[short_labels.index]\n",
    "\n",
    "labels.to_csv(\"data/preprocessing/regression_labels.csv\", index=False)\n",
    "\n",
    "labels = labels.to_numpy()\n",
    "short_labels = short_labels.to_numpy()\n",
    "\n",
    "print(labels.shape)\n",
    "print(short_labels.shape)\n",
    "assert labels.shape[0] == preprocessed_data.shape[0]\n",
    "assert short_labels.shape[0] == short_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(701325, 8) (175332, 8) (701325, 1) (175332, 1)\n",
      "(28468, 8) (7117, 8) (28468, 1) (7117, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "STATE = 42\n",
    "\n",
    "X = preprocessed_data\n",
    "y = labels\n",
    "\n",
    "short_X = short_data\n",
    "short_y = short_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "short_X_train, short_X_test, short_y_train, short_y_test = train_test_split(\n",
    "    short_X, short_y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(short_X_train.shape, short_X_test.shape, short_y_train.shape, short_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "\n",
    "#### Finding best plane\n",
    "\n",
    "- Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BaseEstimator,\n",
    "    X_train: np.ndarray,\n",
    "    y_train: np.ndarray,\n",
    "    X_test: np.ndarray,\n",
    "    y_test: np.ndarray,\n",
    ") -> None:\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"R2: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "MSE: 1669969.90\n",
      "R2: 0.07\n",
      "\n",
      "Short Linear Regression\n",
      "MSE: 188.63\n",
      "R2: 0.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "short_model = LinearRegression()\n",
    "\n",
    "print(\"Linear Regression\")\n",
    "evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort Linear Regression\")\n",
    "evaluate(short_model, short_X_train, short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Stochastic Gradient Descent (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD Regressor\n",
      "MSE: 1674739.81\n",
      "R2: 0.06\n",
      "\n",
      "Short SGD Regressor\n",
      "MSE: 188.97\n",
      "R2: 0.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "reshaped_y_train = y_train.ravel()\n",
    "model = SGDRegressor(\n",
    "    max_iter=1000, tol=1e-3, penalty=\"l2\", alpha=0.0001, learning_rate=\"invscaling\", random_state=STATE\n",
    ")\n",
    "\n",
    "reshaped_short_y_train = short_y_train.ravel()\n",
    "short_model = SGDRegressor(\n",
    "    max_iter=1000, tol=1e-3, penalty=\"l2\", alpha=0.0001, learning_rate=\"invscaling\", random_state=STATE\n",
    ")\n",
    "\n",
    "print(\"SGD Regressor\")\n",
    "evaluate(model, X_train, reshaped_y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort SGD Regressor\")\n",
    "evaluate(short_model, short_X_train, reshaped_short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge\n",
      "MSE: 1669969.91\n",
      "R2: 0.07\n",
      "\n",
      "Short Ridge\n",
      "MSE: 188.63\n",
      "R2: 0.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "model = Ridge(alpha=1.0, random_state=STATE)\n",
    "short_model = Ridge(alpha=1.0, random_state=STATE)\n",
    "\n",
    "print(\"Ridge\")\n",
    "evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort Ridge\")\n",
    "evaluate(short_model, short_X_train, short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso\n",
      "MSE: 1669971.57\n",
      "R2: 0.07\n",
      "\n",
      "Short Lasso\n",
      "MSE: 188.71\n",
      "R2: 0.08\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha=0.1, random_state=STATE)\n",
    "short_model = Lasso(alpha=0.1, random_state=STATE)\n",
    "\n",
    "print(\"Lasso\")\n",
    "evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort Lasso\")\n",
    "evaluate(short_model, short_X_train, short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Elastic Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ElasticNet\n",
      "MSE: 1670393.48\n",
      "R2: 0.07\n",
      "\n",
      "Short ElasticNet\n",
      "MSE: 189.45\n",
      "R2: 0.07\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=STATE)\n",
    "short_model = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=STATE)\n",
    "\n",
    "print(\"ElasticNet\")\n",
    "evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort ElasticNet\")\n",
    "evaluate(short_model, short_X_train, short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree\n",
      "MSE: 2703625.33\n",
      "R2: -0.51\n",
      "\n",
      "Short Decision Tree\n",
      "MSE: 377.83\n",
      "R2: -0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=STATE)\n",
    "short_model = DecisionTreeRegressor(random_state=STATE)\n",
    "\n",
    "print(\"Decision Tree\")\n",
    "evaluate(model, X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort Decision Tree\")\n",
    "evaluate(short_model, short_X_train, short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "MSE: 1398867.68\n",
      "R2: 0.22\n",
      "\n",
      "Short Random Forest\n",
      "MSE: 191.62\n",
      "R2: 0.06\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=STATE)\n",
    "short_model = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=STATE)\n",
    "\n",
    "print(\"Random Forest\")\n",
    "evaluate(model, X_train, reshaped_y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort Random Forest\")\n",
    "evaluate(short_model, short_X_train, reshaped_short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting\n",
      "MSE: 1375759.40\n",
      "R2: 0.23\n",
      "\n",
      "Short Gradient Boosting\n",
      "MSE: 185.45\n",
      "R2: 0.09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=STATE)\n",
    "short_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, random_state=STATE)\n",
    "\n",
    "print(\"Gradient Boosting\")\n",
    "evaluate(model, X_train, reshaped_y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nShort Gradient Boosting\")\n",
    "evaluate(short_model, short_X_train, reshaped_short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Short SVR\n",
      "MSE: 194.53\n",
      "R2: 0.05\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "short_model = SVR(kernel=\"rbf\", C=100, gamma=0.1, epsilon=0.1)\n",
    "\n",
    "print(\"Short SVR\")\n",
    "evaluate(short_model, short_X_train, reshaped_short_y_train, short_X_test, short_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron Regressor (MLP)\n",
    "\n",
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "num_epochs = 10000\n",
    "batch_size = 256\n",
    "\n",
    "\n",
    "class RegressionModel(nn.Module):\n",
    "    PATH = \"data/models/\"\n",
    "    EXT = \".pth\"\n",
    "\n",
    "    def __init__(self, input_dim: int, state_file: str = None):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, 1)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.state_loaded = False\n",
    "\n",
    "        if state_file:\n",
    "            self.load_state_dict(torch.load(state_file))\n",
    "            self.state_loaded = True\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "    def train_model(\n",
    "        self, X_train: np.ndarray, y_train: np.ndarray, file_name: str\n",
    "    ) -> None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Training on device: {device}\")\n",
    "        self.to(device)\n",
    "\n",
    "        self.state_loaded = False\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32).to(device),\n",
    "            torch.tensor(y_train, dtype=torch.float32).to(device),\n",
    "        )\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        scaler = GradScaler()\n",
    "\n",
    "        best_loss = float(\"inf\")\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        min_delta = 1e-6\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            for batch_X, batch_y in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                with autocast():\n",
    "                    outputs = self(batch_X)\n",
    "                    loss = self.criterion(outputs, batch_y)\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "            avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "            if epoch % 1000 == 0:\n",
    "                checkpoint_path = f\"{self.PATH}{file_name}_epoch_{epoch}{self.EXT}\"\n",
    "                torch.save(self.state_dict(), checkpoint_path)\n",
    "                print(f\"Checkpoint saved at epoch {epoch} to {checkpoint_path}\")\n",
    "\n",
    "            if best_loss - min_delta < avg_loss < best_loss:\n",
    "                patience_counter += 1\n",
    "            else:\n",
    "                patience_counter = 0\n",
    "                best_loss = min(best_loss, avg_loss)\n",
    "\n",
    "            if patience_counter > patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "        self.state_loaded = True\n",
    "        print(f\"Epoch {num_epochs}, Loss: {avg_loss}\")\n",
    "        torch.save(self.state_dict(), self.PATH + file_name + self.EXT)\n",
    "\n",
    "    def test_model(\n",
    "        self, X_test: np.ndarray, y_test: np.ndarray, file_name: str | None = None\n",
    "    ) -> None:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"Testing on device: {device}\")\n",
    "        self.to(device)\n",
    "\n",
    "        if not self.state_loaded and file_name:\n",
    "            self.load_state_dict(torch.load(self.PATH + file_name + self.EXT))\n",
    "            self.state_loaded = True\n",
    "\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(X_test, dtype=torch.float32),\n",
    "            torch.tensor(y_test, dtype=torch.float32),\n",
    "        )\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        predictions = []\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            total_loss = 0\n",
    "            total_var = 0\n",
    "            y_mean = torch.mean(torch.tensor(y_test, dtype=torch.float32))\n",
    "            for inputs, targets in test_loader:\n",
    "                outputs = self(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                total_loss += loss.item()\n",
    "                total_var += ((targets - y_mean) ** 2).sum().item()\n",
    "                predictions.extend(outputs.tolist())\n",
    "            r2_score = 1 - (total_loss / total_var)\n",
    "            print(f\"Test Loss: {total_loss / len(test_loader)}\")\n",
    "            print(f\"R2 Score: {r2_score}\")\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short MLP\n",
      "Training on device: cuda\n",
      "Epoch 1/10000, Loss: 224.83201270680792\n",
      "Checkpoint saved at epoch 0 to data/models/short_mlp_epoch_0.pth\n",
      "Epoch 101/10000, Loss: 185.89996898976142\n",
      "Epoch 201/10000, Loss: 181.1987089832802\n",
      "Epoch 301/10000, Loss: 178.86003434390764\n",
      "Epoch 401/10000, Loss: 177.38424887892376\n",
      "Epoch 501/10000, Loss: 176.57157391603752\n",
      "Epoch 601/10000, Loss: 175.73977531125192\n",
      "Epoch 701/10000, Loss: 175.41269420401397\n",
      "Epoch 801/10000, Loss: 174.79321295905007\n",
      "Epoch 901/10000, Loss: 174.5734983709361\n",
      "Epoch 1001/10000, Loss: 174.05061169269374\n",
      "Checkpoint saved at epoch 1000 to data/models/short_mlp_epoch_1000.pth\n",
      "Epoch 1101/10000, Loss: 174.25412048887245\n",
      "Epoch 1201/10000, Loss: 173.82308662311914\n",
      "Epoch 1301/10000, Loss: 173.73604005548452\n",
      "Epoch 1401/10000, Loss: 173.3585079176009\n",
      "Epoch 1501/10000, Loss: 173.62924358556089\n",
      "Epoch 1601/10000, Loss: 172.8890862571819\n",
      "Epoch 1701/10000, Loss: 173.60251620852893\n",
      "Epoch 1801/10000, Loss: 173.27438641877453\n",
      "Epoch 1901/10000, Loss: 173.36735657833083\n",
      "Epoch 2001/10000, Loss: 172.9971135913524\n",
      "Checkpoint saved at epoch 2000 to data/models/short_mlp_epoch_2000.pth\n",
      "Epoch 2101/10000, Loss: 172.78501187311693\n",
      "Epoch 2201/10000, Loss: 172.7440661785314\n",
      "Epoch 2301/10000, Loss: 172.83706589771492\n",
      "Epoch 2401/10000, Loss: 172.84529531162417\n",
      "Epoch 2501/10000, Loss: 172.60185467501927\n",
      "Epoch 2601/10000, Loss: 172.50401518270039\n",
      "Epoch 2701/10000, Loss: 172.53323897973306\n",
      "Epoch 2801/10000, Loss: 172.47043626725406\n",
      "Epoch 2901/10000, Loss: 172.52528880850616\n",
      "Epoch 3001/10000, Loss: 172.0663912649112\n",
      "Checkpoint saved at epoch 3000 to data/models/short_mlp_epoch_3000.pth\n",
      "Epoch 3101/10000, Loss: 172.31646317965246\n",
      "Epoch 3201/10000, Loss: 172.21675917279026\n",
      "Epoch 3301/10000, Loss: 172.16778239434075\n",
      "Epoch 3401/10000, Loss: 172.36968809392954\n",
      "Epoch 3501/10000, Loss: 172.3159831778351\n",
      "Epoch 3601/10000, Loss: 172.258407113798\n",
      "Epoch 3701/10000, Loss: 171.975566949545\n",
      "Epoch 3801/10000, Loss: 172.35677443705333\n",
      "Epoch 3901/10000, Loss: 172.24151050242608\n",
      "Epoch 4001/10000, Loss: 171.80978978588976\n",
      "Checkpoint saved at epoch 4000 to data/models/short_mlp_epoch_4000.pth\n",
      "Epoch 4101/10000, Loss: 171.8652189793608\n",
      "Epoch 4201/10000, Loss: 171.96347921739246\n",
      "Epoch 4301/10000, Loss: 172.12190663974917\n",
      "Epoch 4401/10000, Loss: 172.00865433355082\n",
      "Epoch 4501/10000, Loss: 172.09399123255983\n",
      "Epoch 4601/10000, Loss: 171.95813426201653\n",
      "Epoch 4701/10000, Loss: 172.17658401283984\n",
      "Epoch 4801/10000, Loss: 172.06764693324342\n",
      "Epoch 4901/10000, Loss: 171.89968844700286\n",
      "Epoch 5001/10000, Loss: 172.03695107361662\n",
      "Checkpoint saved at epoch 5000 to data/models/short_mlp_epoch_5000.pth\n",
      "Epoch 5101/10000, Loss: 172.0601853169668\n",
      "Epoch 5201/10000, Loss: 171.9557355530059\n",
      "Epoch 5301/10000, Loss: 171.5982685516768\n",
      "Epoch 5401/10000, Loss: 171.6492721489192\n",
      "Epoch 5501/10000, Loss: 172.01028154997547\n",
      "Epoch 5601/10000, Loss: 171.98056392926273\n",
      "Epoch 5701/10000, Loss: 172.05002043172382\n",
      "Epoch 5801/10000, Loss: 171.71277642356975\n",
      "Epoch 5901/10000, Loss: 171.8266703515844\n",
      "Epoch 6001/10000, Loss: 171.79194052657738\n",
      "Checkpoint saved at epoch 6000 to data/models/short_mlp_epoch_6000.pth\n",
      "Epoch 6101/10000, Loss: 171.74712484727527\n",
      "Epoch 6201/10000, Loss: 171.79421319662188\n",
      "Epoch 6301/10000, Loss: 171.68957197933454\n",
      "Epoch 6401/10000, Loss: 171.78043136254556\n",
      "Epoch 6501/10000, Loss: 171.33807537267026\n",
      "Epoch 6601/10000, Loss: 171.64515946050395\n",
      "Epoch 6701/10000, Loss: 172.15893027814514\n",
      "Epoch 6801/10000, Loss: 171.66268592458135\n",
      "Epoch 6901/10000, Loss: 173.83735779903395\n",
      "Epoch 7001/10000, Loss: 171.53874165487932\n",
      "Checkpoint saved at epoch 7000 to data/models/short_mlp_epoch_7000.pth\n",
      "Epoch 7101/10000, Loss: 171.9015363684684\n",
      "Epoch 7201/10000, Loss: 171.62982362482046\n",
      "Epoch 7301/10000, Loss: 171.84558098626243\n",
      "Epoch 7401/10000, Loss: 171.7988762962444\n",
      "Epoch 7501/10000, Loss: 171.89787826195962\n",
      "Epoch 7601/10000, Loss: 171.86390829727787\n",
      "Epoch 7701/10000, Loss: 171.58432451598847\n",
      "Epoch 7801/10000, Loss: 171.68293611671893\n",
      "Epoch 7901/10000, Loss: 171.41092267913135\n",
      "Epoch 8001/10000, Loss: 171.84670425209765\n",
      "Checkpoint saved at epoch 8000 to data/models/short_mlp_epoch_8000.pth\n",
      "Epoch 8101/10000, Loss: 171.63689850264066\n",
      "Epoch 8201/10000, Loss: 172.02614056369114\n",
      "Epoch 8301/10000, Loss: 171.9096740585806\n",
      "Epoch 8401/10000, Loss: 171.59115655325988\n",
      "Epoch 8501/10000, Loss: 171.80875906495234\n",
      "Epoch 8601/10000, Loss: 171.89937601816493\n",
      "Epoch 8701/10000, Loss: 171.92592716644697\n",
      "Epoch 8801/10000, Loss: 171.56155005484953\n",
      "Epoch 8901/10000, Loss: 171.95308627141432\n",
      "Epoch 9001/10000, Loss: 171.80732295972885\n",
      "Checkpoint saved at epoch 9000 to data/models/short_mlp_epoch_9000.pth\n",
      "Epoch 9101/10000, Loss: 171.4895325391282\n",
      "Epoch 9201/10000, Loss: 172.08458589972938\n",
      "Epoch 9301/10000, Loss: 171.47326892801465\n",
      "Epoch 9401/10000, Loss: 171.43720529325338\n",
      "Epoch 9501/10000, Loss: 171.48496372924257\n",
      "Epoch 9601/10000, Loss: 171.48838556400864\n",
      "Epoch 9701/10000, Loss: 171.5810824680756\n",
      "Epoch 9801/10000, Loss: 171.96937472082575\n",
      "Epoch 9901/10000, Loss: 171.75167080318982\n",
      "Epoch 10000, Loss: 171.46149796969152\n"
     ]
    }
   ],
   "source": [
    "short_mlp = RegressionModel(short_X_train.shape[1])\n",
    "\n",
    "short_y_train_numeric = short_y_train.astype(np.float32)\n",
    "\n",
    "print(\"\\nShort MLP\")\n",
    "short_mlp.train_model(short_X_train, short_y_train_numeric, \"short_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "Training on device: cuda\n",
      "Epoch 1/10000, Loss: 1549989.9737397353\n",
      "Checkpoint saved at epoch 0 to data/models/regression_mlp_epoch_0.pth\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m y_train_numeric \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLP\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_numeric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregression_mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 63\u001b[0m, in \u001b[0;36mRegressionModel.train_model\u001b[1;34m(self, X_train, y_train, file_name)\u001b[0m\n\u001b[0;32m     61\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(batch_X)\n\u001b[0;32m     62\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, batch_y)\n\u001b[1;32m---> 63\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[0;32m     65\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[1;32mc:\\Users\\gurgel\\Desktop\\Personal\\Projects\\UFF\\Mestrado\\Machine Learning\\Project\\.venv\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurgel\\Desktop\\Personal\\Projects\\UFF\\Mestrado\\Machine Learning\\Project\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gurgel\\Desktop\\Personal\\Projects\\UFF\\Mestrado\\Machine Learning\\Project\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mlp = RegressionModel(X_train.shape[1])\n",
    "\n",
    "y_train_numeric = y_train.astype(np.float32)\n",
    "\n",
    "print(\"MLP\")\n",
    "mlp.train_model(X_train, y_train_numeric, \"regression_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Short MLP\n",
      "Testing on device: cpu\n",
      "Test Loss: 219.92981229509627\n",
      "R2 Score: 0.9915258297723392\n"
     ]
    }
   ],
   "source": [
    "short_y_test_numeric = short_y_test.astype(np.float32)\n",
    "\n",
    "print(\"\\nShort MLP\")\n",
    "predictions = short_mlp.test_model(short_X_test, short_y_test_numeric, \"short_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_numeric = y_test.astype(np.float32)\n",
    "\n",
    "print(\"MLP\")\n",
    "mlp.test_model(X_test, y_test_numeric, \"regression_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on device: cpu\n",
      "Test Loss: 219.9199641091483\n",
      "R2 Score: 0.9915262093491144\n"
     ]
    }
   ],
   "source": [
    "model = RegressionModel(short_X_train.shape[1], \"data/models/short_mlp.pth\")\n",
    "short_y_test_numeric = short_y_test.astype(np.float32)\n",
    "predictions = model.test_model(short_X_test, short_y_test_numeric, \"short_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual  predicted\n",
      "0     26  23.924051\n",
      "1     48  -0.223848\n",
      "2     47  22.442184\n",
      "3     38  26.791649\n",
      "4     17   1.720243\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\"actual\": short_y_test.flatten(), \"predicted\": np.array(predictions).flatten()}\n",
    ")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"data/predictions/regression_mlp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on device: cpu\n",
      "Test Loss: 3014959.9076642334\n",
      "R2 Score: 0.9868281109962662\n"
     ]
    }
   ],
   "source": [
    "full_model = RegressionModel(short_X_train.shape[1], \"data/models/short_mlp.pth\")\n",
    "y_test_numeric = y_test.astype(np.float32)\n",
    "predictions = full_model.test_model(X_test, y_test_numeric, \"short_mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual  predicted\n",
      "0   4308  25.021648\n",
      "1   1777  24.802710\n",
      "2   1438  27.992088\n",
      "3    284  30.588078\n",
      "4    471  23.378174\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(\n",
    "    {\"actual\": y_test.flatten(), \"predicted\": np.array(predictions).flatten()}\n",
    ")\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"data/predictions/full_regression_mlp.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
