{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Maintenance with Azure Dataset\n",
    "\n",
    "## Project imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Imports\n",
    "\n",
    "Needs pre-processing.ipynb to be run first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read(name: str, parse_dates: list[str] | None = [\"datetime\"]) -> pd.DataFrame:\n",
    "    path = \"data/\"\n",
    "    ext = \".csv\"\n",
    "    file = path + name + ext\n",
    "    return pd.read_csv(file, parse_dates=parse_dates, na_values=\"NaN\")\n",
    "\n",
    "VARIABLES = [\"volt\", \"rotate\", \"pressure\", \"vibration\"]\n",
    "DATA = read(\"raw_data\").dropna(subset=VARIABLES)\n",
    "normal_behavior_data = read(\"preprocessing/expected_behavior\")\n",
    "abnormal_data = read(\"preprocessing/failures_only\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset classification\n",
    "\n",
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877209, 6)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "\n",
    "STATE = 42\n",
    "\n",
    "independent_data = DATA.drop(\n",
    "    columns=[\"datetime\", \"machineID\", \"errorID\", \"failure\", \"comp\"]\n",
    ")\n",
    "labels = DATA[[\"errorID\", \"failure\", \"comp\"]]\n",
    "\n",
    "numeric_features = [\"volt\", \"rotate\", \"pressure\", \"vibration\", \"age\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"model\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "        (\"ordinal\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessed_data = preprocessor.fit_transform(independent_data)\n",
    "print(preprocessed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for errorID:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Anomaly       1.00      1.00      1.00    174705\n",
      "      error1       0.00      0.00      0.00       204\n",
      "      error2       0.33      0.01      0.01       179\n",
      "      error3       0.00      0.00      0.00       156\n",
      "      error4       0.00      0.00      0.00       134\n",
      "      error5       1.00      0.00      0.00        64\n",
      "\n",
      "    accuracy                           1.00    175442\n",
      "   macro avg       0.39      0.17      0.17    175442\n",
      "weighted avg       0.99      1.00      0.99    175442\n",
      "\n",
      "Classification report for failure:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Anomaly       1.00      1.00      1.00    175216\n",
      "       comp1       0.00      0.00      0.00        64\n",
      "       comp2       0.00      0.00      0.00        69\n",
      "       comp3       0.00      0.00      0.00        39\n",
      "       comp4       0.00      0.00      0.00        54\n",
      "\n",
      "    accuracy                           1.00    175442\n",
      "   macro avg       0.20      0.20      0.20    175442\n",
      "weighted avg       1.00      1.00      1.00    175442\n",
      "\n",
      "Classification report for comp:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Anomaly       1.00      1.00      1.00    174838\n",
      "       comp1       0.00      0.00      0.00       139\n",
      "       comp2       0.00      0.00      0.00       154\n",
      "       comp3       0.00      0.00      0.00       160\n",
      "       comp4       0.00      0.00      0.00       151\n",
      "\n",
      "    accuracy                           1.00    175442\n",
      "   macro avg       0.20      0.20      0.20    175442\n",
      "weighted avg       0.99      1.00      0.99    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = preprocessed_data\n",
    "\n",
    "# Run the classifier for each label\n",
    "for label in [\"errorID\", \"failure\", \"comp\"]:\n",
    "    print(f\"Classification report for {label}:\")\n",
    "    y = labels[label].fillna(\"No Anomaly\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=STATE\n",
    "    )\n",
    "\n",
    "    # Train a KNN classifier\n",
    "    knn = KNeighborsClassifier(n_neighbors=3, n_jobs=-1)\n",
    "    knn.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate the classifier\n",
    "    y_pred = knn.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred, zero_division=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Narrowing classification to specific cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Anomaly       0.99      1.00      1.00    174104\n",
      "        comp       0.00      0.00      0.00       379\n",
      "     errorID       0.00      0.00      0.00       733\n",
      "     failure       0.33      0.00      0.01       226\n",
      "\n",
      "    accuracy                           0.99    175442\n",
      "   macro avg       0.33      0.25      0.25    175442\n",
      "weighted avg       0.99      0.99      0.99    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the labels into a single label\n",
    "def label(row: pd.Series) -> str:\n",
    "    if not pd.isna(row[\"failure\"]):\n",
    "        return \"failure\"\n",
    "    if not pd.isna(row[\"errorID\"]):\n",
    "        return \"errorID\"\n",
    "    if not pd.isna(row[\"comp\"]):\n",
    "        return \"comp\"\n",
    "    return \"No Anomaly\"\n",
    "\n",
    "\n",
    "labels_combined = DATA.apply(label, axis=1)\n",
    "\n",
    "X = preprocessed_data\n",
    "y = labels_combined\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.42      0.02      0.04      1338\n",
      "  No Anomaly       0.99      1.00      1.00    174104\n",
      "\n",
      "    accuracy                           0.99    175442\n",
      "   macro avg       0.71      0.51      0.52    175442\n",
      "weighted avg       0.99      0.99      0.99    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Combine the labels into a single binary label\n",
    "binary_labels = labels.any(axis=1).replace({False: \"No Anomaly\", True: \"Anomaly\"})\n",
    "\n",
    "X = preprocessed_data\n",
    "y = binary_labels\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.01      0.42      0.02      1338\n",
      "  No Anomaly       0.99      0.67      0.80    174104\n",
      "\n",
      "    accuracy                           0.67    175442\n",
      "   macro avg       0.50      0.55      0.41    175442\n",
      "weighted avg       0.99      0.67      0.80    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svc_sgd = SGDClassifier(\n",
    "    loss=\"hinge\", class_weight=\"balanced\", random_state=STATE, n_jobs=-1\n",
    ")\n",
    "svc_sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_sgd.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.01      0.73      0.01      1338\n",
      "  No Anomaly       0.99      0.23      0.37    174104\n",
      "\n",
      "    accuracy                           0.23    175442\n",
      "   macro avg       0.50      0.48      0.19    175442\n",
      "weighted avg       0.98      0.23      0.37    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_sgd = SGDClassifier(\n",
    "    loss=\"perceptron\", class_weight=\"balanced\", random_state=STATE, n_jobs=-1\n",
    ")\n",
    "svc_sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_sgd.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.00      0.00      0.00      1338\n",
      "  No Anomaly       0.99      1.00      1.00    174104\n",
      "\n",
      "    accuracy                           0.99    175442\n",
      "   macro avg       0.50      0.50      0.50    175442\n",
      "weighted avg       0.98      0.99      0.99    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "slp = MLPClassifier(hidden_layer_sizes=(1,), random_state=STATE)\n",
    "slp.fit(X_train, y_train)\n",
    "\n",
    "y_pred = slp.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.01      0.70      0.02      1338\n",
      "  No Anomaly       0.99      0.34      0.51    174104\n",
      "\n",
      "    accuracy                           0.34    175442\n",
      "   macro avg       0.50      0.52      0.26    175442\n",
      "weighted avg       0.99      0.34      0.50    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc_sgd = SGDClassifier(\n",
    "    loss=\"log_loss\", class_weight=\"balanced\", random_state=STATE, n_jobs=-1\n",
    ")\n",
    "svc_sgd.fit(X_train, y_train)\n",
    "\n",
    "y_pred = svc_sgd.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Anomaly       0.01      0.52      0.02      1338\n",
      "  No Anomaly       0.99      0.56      0.72    174104\n",
      "\n",
      "    accuracy                           0.56    175442\n",
      "   macro avg       0.50      0.54      0.37    175442\n",
      "weighted avg       0.99      0.56      0.71    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(class_weight=\"balanced\", random_state=STATE, n_jobs=-1)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  No Anomaly       0.99      1.00      1.00    174104\n",
      "        comp       1.00      0.34      0.50       379\n",
      "     errorID       1.00      0.12      0.21       733\n",
      "     failure       1.00      0.60      0.75       226\n",
      "\n",
      "    accuracy                           0.99    175442\n",
      "   macro avg       1.00      0.51      0.62    175442\n",
      "weighted avg       0.99      0.99      0.99    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X = preprocessed_data\n",
    "y = labels_combined\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Failure data classification\n",
    "\n",
    "### Pre-processing\n",
    "\n",
    "- Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877209, 8)\n"
     ]
    }
   ],
   "source": [
    "independent_data = DATA.drop(\n",
    "    columns=[\"datetime\", \"machineID\", \"failure\"]\n",
    ")\n",
    "\n",
    "numeric_features = [\"volt\", \"rotate\", \"pressure\", \"vibration\", \"age\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"model\", \"errorID\", \"comp\"]\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"None\")),\n",
    "        (\"ordinal\", OrdinalEncoder()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessed_data = preprocessor.fit_transform(independent_data)\n",
    "print(preprocessed_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877209, 1)\n",
      "failure                    \n",
      "normal                         841624\n",
      "25_to_30_hours_before_comp2      1500\n",
      "7_to_12_hours_before_comp2       1500\n",
      "43_to_48_hours_before_comp2      1500\n",
      "19_to_24_hours_before_comp2      1500\n",
      "37_to_42_hours_before_comp2      1500\n",
      "13_to_18_hours_before_comp2      1500\n",
      "1_to_6_hours_before_comp2        1500\n",
      "31_to_36_hours_before_comp2      1500\n",
      "7_to_12_hours_before_comp1       1152\n",
      "37_to_42_hours_before_comp1      1152\n",
      "31_to_36_hours_before_comp1      1152\n",
      "13_to_18_hours_before_comp1      1152\n",
      "25_to_30_hours_before_comp1      1152\n",
      "1_to_6_hours_before_comp1        1152\n",
      "19_to_24_hours_before_comp1      1152\n",
      "43_to_48_hours_before_comp1      1149\n",
      "1_to_6_hours_before_comp4         930\n",
      "13_to_18_hours_before_comp4       930\n",
      "25_to_30_hours_before_comp4       930\n",
      "31_to_36_hours_before_comp4       930\n",
      "19_to_24_hours_before_comp4       930\n",
      "7_to_12_hours_before_comp4        930\n",
      "37_to_42_hours_before_comp4       930\n",
      "43_to_48_hours_before_comp4       930\n",
      "1_to_6_hours_before_comp3         728\n",
      "7_to_12_hours_before_comp3        726\n",
      "13_to_18_hours_before_comp3       726\n",
      "43_to_48_hours_before_comp3       726\n",
      "25_to_30_hours_before_comp3       726\n",
      "37_to_42_hours_before_comp3       726\n",
      "19_to_24_hours_before_comp3       726\n",
      "31_to_36_hours_before_comp3       726\n",
      "comp2                             386\n",
      "comp1                             291\n",
      "comp4                             255\n",
      "comp3                             190\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "(877209, 1)\n",
      "failure    \n",
      "normal         841624\n",
      "pre-failure     34463\n",
      "failure          1122\n",
      "Name: count, dtype: int64\n",
      "(877209, 1)\n",
      "failure\n",
      "normal     841624\n",
      "comp2       12386\n",
      "comp1        9504\n",
      "comp4        7695\n",
      "comp3        6000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "time_before_failure = 48\n",
    "interval = 6\n",
    "counter = 0\n",
    "\n",
    "labels = DATA[[\"failure\"]].fillna(\"normal\").reset_index(drop=True)\n",
    "binary_labels = DATA[[\"failure\"]].fillna(\"normal\").reset_index(drop=True)\n",
    "interval_labels = DATA[[\"failure\"]].fillna(\"normal\").reset_index(drop=True)\n",
    "\n",
    "for i in range(len(labels) - 1, -1, -1):\n",
    "    if labels.at[i, \"failure\"] != \"normal\":\n",
    "        counter = time_before_failure\n",
    "        binary_labels.at[i, \"failure\"] = \"failure\"\n",
    "        current_failure = labels.at[i, \"failure\"]\n",
    "\n",
    "    elif counter > 0:\n",
    "        counter -= 1\n",
    "\n",
    "        first_hour = (\n",
    "            f\"{time_before_failure - interval - counter // interval * interval + 1}\"\n",
    "        )\n",
    "        last_hour = f\"{time_before_failure - counter // interval * interval}\"\n",
    "        label = f\"{first_hour}_to_{last_hour}_hours_before_{current_failure}\"\n",
    "\n",
    "        labels.at[i, \"failure\"] = label\n",
    "        binary_labels.at[i, \"failure\"] = \"pre-failure\"\n",
    "        interval_labels.at[i, \"failure\"] = current_failure\n",
    "\n",
    "\n",
    "binary_labels.to_csv(\"data/preprocessing/binary_labels.csv\", index=False)\n",
    "labels.to_csv(\"data/preprocessing/labels.csv\", index=False)\n",
    "interval_labels.to_csv(\"data/preprocessing/interval_labels.csv\", index=False)\n",
    "\n",
    "print(labels.shape)\n",
    "print(f\"{labels.value_counts()}\\n\\n\")\n",
    "print(binary_labels.shape)\n",
    "print(binary_labels.value_counts())\n",
    "print(interval_labels.shape)\n",
    "print(interval_labels.value_counts())\n",
    "\n",
    "assert labels.shape[0] == preprocessed_data.shape[0]\n",
    "assert labels.shape == binary_labels.shape\n",
    "assert labels.shape == interval_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "- Categorical failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "13_to_18_hours_before_comp1       0.00      0.00      0.00       244\n",
      "13_to_18_hours_before_comp2       0.00      0.00      0.00       267\n",
      "13_to_18_hours_before_comp3       0.00      0.00      0.00       170\n",
      "13_to_18_hours_before_comp4       0.00      0.00      0.00       175\n",
      "19_to_24_hours_before_comp1       0.00      0.00      0.00       210\n",
      "19_to_24_hours_before_comp2       0.48      0.04      0.08       289\n",
      "19_to_24_hours_before_comp3       0.75      0.12      0.21       148\n",
      "19_to_24_hours_before_comp4       0.81      0.12      0.20       191\n",
      "  1_to_6_hours_before_comp1       0.00      0.00      0.00       232\n",
      "  1_to_6_hours_before_comp2       0.00      0.00      0.00       291\n",
      "  1_to_6_hours_before_comp3       0.00      0.00      0.00       160\n",
      "  1_to_6_hours_before_comp4       0.00      0.00      0.00       179\n",
      "25_to_30_hours_before_comp1       0.00      0.00      0.00       222\n",
      "25_to_30_hours_before_comp2       0.62      0.03      0.06       274\n",
      "25_to_30_hours_before_comp3       0.00      0.00      0.00       123\n",
      "25_to_30_hours_before_comp4       0.00      0.00      0.00       184\n",
      "31_to_36_hours_before_comp1       0.00      0.00      0.00       233\n",
      "31_to_36_hours_before_comp2       0.00      0.00      0.00       290\n",
      "31_to_36_hours_before_comp3       0.00      0.00      0.00       142\n",
      "31_to_36_hours_before_comp4       0.00      0.00      0.00       184\n",
      "37_to_42_hours_before_comp1       0.00      0.00      0.00       248\n",
      "37_to_42_hours_before_comp2       0.00      0.00      0.00       314\n",
      "37_to_42_hours_before_comp3       0.00      0.00      0.00       141\n",
      "37_to_42_hours_before_comp4       0.00      0.00      0.00       193\n",
      "43_to_48_hours_before_comp1       0.00      0.00      0.00       208\n",
      "43_to_48_hours_before_comp2       0.00      0.00      0.00       295\n",
      "43_to_48_hours_before_comp3       0.00      0.00      0.00       151\n",
      "43_to_48_hours_before_comp4       0.00      0.00      0.00       160\n",
      " 7_to_12_hours_before_comp1       0.00      0.00      0.00       236\n",
      " 7_to_12_hours_before_comp2       0.00      0.00      0.00       318\n",
      " 7_to_12_hours_before_comp3       0.00      0.00      0.00       128\n",
      " 7_to_12_hours_before_comp4       0.00      0.00      0.00       165\n",
      "                      comp1       0.00      0.00      0.00        64\n",
      "                      comp2       0.65      0.29      0.40        69\n",
      "                      comp3       0.68      0.59      0.63        39\n",
      "                      comp4       0.86      0.56      0.67        54\n",
      "                     normal       0.96      1.00      0.98    168451\n",
      "\n",
      "                   accuracy                           0.96    175442\n",
      "                  macro avg       0.16      0.07      0.09    175442\n",
      "               weighted avg       0.93      0.96      0.94    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed_data\n",
    "y = labels[\"failure\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=23, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Binary failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     failure       0.81      0.53      0.64       226\n",
      "      normal       0.96      1.00      0.98    168451\n",
      " pre-failure       0.45      0.07      0.12      6765\n",
      "\n",
      "    accuracy                           0.96    175442\n",
      "   macro avg       0.74      0.53      0.58    175442\n",
      "weighted avg       0.94      0.96      0.95    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed_data\n",
    "y = binary_labels[\"failure\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "knn = KNeighborsClassifier(n_neighbors=7, n_jobs=-1)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "\n",
    "# X = preprocessed_data\n",
    "# y = binary_labels[\"failure\"]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, test_size=0.2, random_state=STATE\n",
    "# )\n",
    "\n",
    "# svc = SVC(\n",
    "#     kernel=\"rbf\",\n",
    "#     class_weight={\"failure\": 1, \"normal\": 1, \"pre-failure\": 3},\n",
    "#     random_state=STATE,\n",
    "# )\n",
    "# svc.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = svc.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             precision    recall  f1-score   support\n",
      "\n",
      "13_to_18_hours_before_comp1       0.00      0.00      0.00       244\n",
      "13_to_18_hours_before_comp2       0.00      0.00      0.00       267\n",
      "13_to_18_hours_before_comp3       0.00      0.00      0.00       170\n",
      "13_to_18_hours_before_comp4       0.00      0.00      0.00       175\n",
      "19_to_24_hours_before_comp1       0.12      0.00      0.01       210\n",
      "19_to_24_hours_before_comp2       0.70      0.05      0.09       289\n",
      "19_to_24_hours_before_comp3       0.79      0.16      0.26       148\n",
      "19_to_24_hours_before_comp4       0.85      0.12      0.20       191\n",
      "  1_to_6_hours_before_comp1       0.00      0.00      0.00       232\n",
      "  1_to_6_hours_before_comp2       0.00      0.00      0.00       291\n",
      "  1_to_6_hours_before_comp3       0.00      0.00      0.00       160\n",
      "  1_to_6_hours_before_comp4       0.00      0.00      0.00       179\n",
      "25_to_30_hours_before_comp1       0.00      0.00      0.00       222\n",
      "25_to_30_hours_before_comp2       0.67      0.08      0.14       274\n",
      "25_to_30_hours_before_comp3       0.00      0.00      0.00       123\n",
      "25_to_30_hours_before_comp4       0.00      0.00      0.00       184\n",
      "31_to_36_hours_before_comp1       0.00      0.00      0.00       233\n",
      "31_to_36_hours_before_comp2       0.00      0.00      0.00       290\n",
      "31_to_36_hours_before_comp3       0.00      0.00      0.00       142\n",
      "31_to_36_hours_before_comp4       0.00      0.00      0.00       184\n",
      "37_to_42_hours_before_comp1       0.00      0.00      0.00       248\n",
      "37_to_42_hours_before_comp2       0.00      0.00      0.00       314\n",
      "37_to_42_hours_before_comp3       0.00      0.00      0.00       141\n",
      "37_to_42_hours_before_comp4       0.00      0.00      0.00       193\n",
      "43_to_48_hours_before_comp1       0.00      0.00      0.00       208\n",
      "43_to_48_hours_before_comp2       0.00      0.00      0.00       295\n",
      "43_to_48_hours_before_comp3       0.00      0.00      0.00       151\n",
      "43_to_48_hours_before_comp4       0.00      0.00      0.00       160\n",
      " 7_to_12_hours_before_comp1       0.00      0.00      0.00       236\n",
      " 7_to_12_hours_before_comp2       0.00      0.00      0.00       318\n",
      " 7_to_12_hours_before_comp3       0.00      0.00      0.00       128\n",
      " 7_to_12_hours_before_comp4       0.00      0.00      0.00       165\n",
      "                      comp1       0.50      0.16      0.24        64\n",
      "                      comp2       0.63      0.48      0.55        69\n",
      "                      comp3       0.76      0.67      0.71        39\n",
      "                      comp4       0.67      0.56      0.61        54\n",
      "                     normal       0.96      1.00      0.98    168451\n",
      "\n",
      "                   accuracy                           0.96    175442\n",
      "                  macro avg       0.18      0.09      0.10    175442\n",
      "               weighted avg       0.93      0.96      0.94    175442\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = preprocessed_data\n",
    "y = labels[\"failure\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(class_weight=\"balanced\", random_state=STATE, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "- Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "class ClassificationModel(nn.Module):\n",
    "    PATH = \"data/models/\"\n",
    "    EXT = \".pth\"\n",
    "\n",
    "    def __init__(self, input_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.output = nn.Linear(64, output_dim)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        self.state_loaded = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.output(x)\n",
    "        return x if self.training else torch.softmax(x, dim=1)\n",
    "\n",
    "    def train_model(\n",
    "        self, X_train: np.ndarray, y_train: np.ndarray, file_name: str\n",
    "    ) -> None:\n",
    "        self.state_loaded = False\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(X_train, dtype=torch.float32),\n",
    "            torch.tensor(y_train, dtype=torch.long),\n",
    "        )\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.train()\n",
    "\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "        self.state_loaded = True\n",
    "        print(f\"Epoch {num_epochs}, Loss: {loss.item()}\")\n",
    "        torch.save(self.state_dict(), self.PATH + file_name + self.EXT)\n",
    "\n",
    "    def test_model(\n",
    "        self, X_test: np.ndarray, y_test: np.ndarray, file_name: str | None = None\n",
    "    ) -> None:\n",
    "        if not self.state_loaded:\n",
    "            self.load_state_dict(torch.load(self.PATH + file_name + self.EXT))\n",
    "            self.state_loaded = True\n",
    "\n",
    "        self.eval()\n",
    "        dataset = TensorDataset(\n",
    "            torch.tensor(X_test, dtype=torch.float32),\n",
    "            torch.tensor(y_test, dtype=torch.long),\n",
    "        )\n",
    "        test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                outputs = self(X_batch)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum().item()\n",
    "            print(f\"Accuracy: {round(100 * correct / total, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.8780949115753174\n",
      "Epoch 11/100, Loss: 0.5379189252853394\n",
      "Epoch 21/100, Loss: 0.6467944383621216\n",
      "Epoch 31/100, Loss: 0.39825063943862915\n",
      "Epoch 41/100, Loss: 0.5953049063682556\n",
      "Epoch 51/100, Loss: 0.44549211859703064\n",
      "Epoch 61/100, Loss: 0.42788803577423096\n",
      "Epoch 71/100, Loss: 0.5345834493637085\n",
      "Epoch 81/100, Loss: 0.3950938582420349\n",
      "Epoch 91/100, Loss: 0.5104618668556213\n",
      "Epoch 100, Loss: 0.3816201388835907\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "filtered_labels = interval_labels[interval_labels[\"failure\"] != \"normal\"]\n",
    "filtered_indices = filtered_labels.index\n",
    "filtered_data = preprocessed_data[filtered_indices]\n",
    "\n",
    "X = filtered_data\n",
    "y = filtered_labels[\"failure\"]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "y_encoded = encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=STATE\n",
    ")\n",
    "\n",
    "model = ClassificationModel(X_train.shape[1], len(y.unique()))\n",
    "model.train_model(X_train, y_train, \"classification_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.5%\n"
     ]
    }
   ],
   "source": [
    "model.test_model(X_test, y_test, \"classification_mlp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
